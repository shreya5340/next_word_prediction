{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cec470",
   "metadata": {},
   "source": [
    "## Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96206d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f272fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a proper human room although a little too small, lay peacefully\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"ref_txt.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "lines[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c522e",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babfee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could s'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6762036a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could s'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5877f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, s'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192d3db",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbec3c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e68ef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac806a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12086b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a66b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31988940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0e780",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b677f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3c64f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             26170     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2617)              2619617   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,694,787\n",
      "Trainable params: 15,694,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba9643",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c8fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2e604",
   "metadata": {},
   "source": [
    "## Compiling and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db44f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.2529\n",
      "Epoch 1: loss improved from 7.33149 to 7.25295, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 172ms/step - loss: 7.2529 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.0991\n",
      "Epoch 2: loss improved from 7.25295 to 7.09909, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 7.0991 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.0114\n",
      "Epoch 3: loss improved from 7.09909 to 7.01141, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 182ms/step - loss: 7.0114 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.9464\n",
      "Epoch 4: loss improved from 7.01141 to 6.94640, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 181ms/step - loss: 6.9464 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.8855\n",
      "Epoch 5: loss improved from 6.94640 to 6.88548, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 6.8855 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.8345\n",
      "Epoch 6: loss improved from 6.88548 to 6.83446, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 168ms/step - loss: 6.8345 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.7758\n",
      "Epoch 7: loss improved from 6.83446 to 6.77576, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 170ms/step - loss: 6.7758 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.7141\n",
      "Epoch 8: loss improved from 6.77576 to 6.71413, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 171ms/step - loss: 6.7141 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.6431\n",
      "Epoch 9: loss improved from 6.71413 to 6.64307, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 168ms/step - loss: 6.6431 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.5929\n",
      "Epoch 10: loss improved from 6.64307 to 6.59293, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 180ms/step - loss: 6.5929 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.5447\n",
      "Epoch 11: loss improved from 6.59293 to 6.54466, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 172ms/step - loss: 6.5447 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.4969\n",
      "Epoch 12: loss improved from 6.54466 to 6.49685, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 6.4969 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.4418\n",
      "Epoch 13: loss improved from 6.49685 to 6.44178, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 171ms/step - loss: 6.4418 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.3921\n",
      "Epoch 14: loss improved from 6.44178 to 6.39212, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 6.3921 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.3539\n",
      "Epoch 15: loss improved from 6.39212 to 6.35387, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 6.3539 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.3096\n",
      "Epoch 16: loss improved from 6.35387 to 6.30959, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 173ms/step - loss: 6.3096 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.2622\n",
      "Epoch 17: loss improved from 6.30959 to 6.26222, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 6.2622 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.2018\n",
      "Epoch 18: loss improved from 6.26222 to 6.20185, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 168ms/step - loss: 6.2018 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.1449\n",
      "Epoch 19: loss improved from 6.20185 to 6.14488, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 176ms/step - loss: 6.1449 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.0725\n",
      "Epoch 20: loss improved from 6.14488 to 6.07251, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 6.0725 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.9504\n",
      "Epoch 21: loss improved from 6.07251 to 5.95042, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 5.9504 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.8135\n",
      "Epoch 22: loss improved from 5.95042 to 5.81346, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 5.8135 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.6735\n",
      "Epoch 23: loss improved from 5.81346 to 5.67348, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 5.6735 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.4970\n",
      "Epoch 24: loss improved from 5.67348 to 5.49698, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 5.4970 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.3280\n",
      "Epoch 25: loss improved from 5.49698 to 5.32798, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 5.3280 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.1652\n",
      "Epoch 26: loss improved from 5.32798 to 5.16519, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 164ms/step - loss: 5.1652 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.9709\n",
      "Epoch 27: loss improved from 5.16519 to 4.97094, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 4.9709 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.7877\n",
      "Epoch 28: loss improved from 4.97094 to 4.78770, saving model to nextword1.h5\n",
      "61/61 [==============================] - 11s 179ms/step - loss: 4.7877 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.6327\n",
      "Epoch 29: loss improved from 4.78770 to 4.63270, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 4.6327 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.4676\n",
      "Epoch 30: loss improved from 4.63270 to 4.46757, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 168ms/step - loss: 4.4676 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.2882\n",
      "Epoch 31: loss improved from 4.46757 to 4.28820, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 4.2882 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.1256\n",
      "Epoch 32: loss improved from 4.28820 to 4.12557, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 4.1256 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.9751\n",
      "Epoch 33: loss improved from 4.12557 to 3.97506, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 3.9751 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.8324\n",
      "Epoch 34: loss improved from 3.97506 to 3.83235, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 3.8324 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.7383\n",
      "Epoch 35: loss improved from 3.83235 to 3.73831, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 8s 129ms/step - loss: 3.7383 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.6262\n",
      "Epoch 36: loss improved from 3.73831 to 3.62621, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 3.6262 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.4764\n",
      "Epoch 37: loss improved from 3.62621 to 3.47641, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 131ms/step - loss: 3.4764 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.3688\n",
      "Epoch 38: loss improved from 3.47641 to 3.36882, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 3.3688 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.3109\n",
      "Epoch 39: loss improved from 3.36882 to 3.31089, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 3.3109 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.1891\n",
      "Epoch 40: loss improved from 3.31089 to 3.18906, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 3.1891 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0998\n",
      "Epoch 41: loss improved from 3.18906 to 3.09983, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 3.0998 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0134\n",
      "Epoch 42: loss improved from 3.09983 to 3.01339, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 3.0134 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.9678\n",
      "Epoch 43: loss improved from 3.01339 to 2.96783, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 2.9678 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.8709\n",
      "Epoch 44: loss improved from 2.96783 to 2.87092, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 2.8709 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.8390\n",
      "Epoch 45: loss improved from 2.87092 to 2.83900, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 2.8390 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.7880\n",
      "Epoch 46: loss improved from 2.83900 to 2.78800, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 2.7880 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.7462\n",
      "Epoch 47: loss improved from 2.78800 to 2.74621, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 2.7462 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6816\n",
      "Epoch 48: loss improved from 2.74621 to 2.68163, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 2.6816 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6206\n",
      "Epoch 49: loss improved from 2.68163 to 2.62061, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 2.6206 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.5567\n",
      "Epoch 50: loss improved from 2.62061 to 2.55669, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 2.5567 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.4834\n",
      "Epoch 51: loss improved from 2.55669 to 2.48340, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 2.4834 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.4444\n",
      "Epoch 52: loss improved from 2.48340 to 2.44436, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 2.4444 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3744\n",
      "Epoch 53: loss improved from 2.44436 to 2.37441, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 2.3744 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3504\n",
      "Epoch 54: loss improved from 2.37441 to 2.35039, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 2.3504 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3168\n",
      "Epoch 55: loss improved from 2.35039 to 2.31678, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 2.3168 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2942\n",
      "Epoch 56: loss improved from 2.31678 to 2.29420, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 2.2942 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2716\n",
      "Epoch 57: loss improved from 2.29420 to 2.27164, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 2.2716 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2486\n",
      "Epoch 58: loss improved from 2.27164 to 2.24865, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 2.2486 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2057\n",
      "Epoch 59: loss improved from 2.24865 to 2.20571, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 2.2057 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1603\n",
      "Epoch 60: loss improved from 2.20571 to 2.16034, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 2.1603 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1118\n",
      "Epoch 61: loss improved from 2.16034 to 2.11178, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 2.1118 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0876\n",
      "Epoch 62: loss improved from 2.11178 to 2.08759, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 2.0876 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0513\n",
      "Epoch 63: loss improved from 2.08759 to 2.05131, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 2.0513 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0270\n",
      "Epoch 64: loss improved from 2.05131 to 2.02705, saving model to nextword1.h5\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 2.0270 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9995\n",
      "Epoch 65: loss improved from 2.02705 to 1.99955, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 1.9995 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9598\n",
      "Epoch 66: loss improved from 1.99955 to 1.95982, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 1.9598 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9208\n",
      "Epoch 67: loss improved from 1.95982 to 1.92076, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 1.9208 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9217\n",
      "Epoch 68: loss did not improve from 1.92076\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 1.9217 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9039\n",
      "Epoch 69: loss improved from 1.92076 to 1.90393, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 1.9039 - lr: 0.0010\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - ETA: 0s - loss: 1.8951\n",
      "Epoch 70: loss improved from 1.90393 to 1.89512, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 1.8951 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8692\n",
      "Epoch 71: loss improved from 1.89512 to 1.86920, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 1.8692 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8448\n",
      "Epoch 72: loss improved from 1.86920 to 1.84483, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 1.8448 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8354\n",
      "Epoch 73: loss improved from 1.84483 to 1.83544, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 1.8354 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7983\n",
      "Epoch 74: loss improved from 1.83544 to 1.79827, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 1.7983 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7787\n",
      "Epoch 75: loss improved from 1.79827 to 1.77871, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 1.7787 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7696\n",
      "Epoch 76: loss improved from 1.77871 to 1.76965, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 1.7696 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7078\n",
      "Epoch 77: loss improved from 1.76965 to 1.70777, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 1.7078 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7267\n",
      "Epoch 78: loss did not improve from 1.70777\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 1.7267 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7036\n",
      "Epoch 79: loss improved from 1.70777 to 1.70362, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 1.7036 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6740\n",
      "Epoch 80: loss improved from 1.70362 to 1.67403, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 1.6740 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6282\n",
      "Epoch 81: loss improved from 1.67403 to 1.62823, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 1.6282 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6373\n",
      "Epoch 82: loss did not improve from 1.62823\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 1.6373 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6312\n",
      "Epoch 83: loss did not improve from 1.62823\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 1.6312 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6265\n",
      "Epoch 84: loss improved from 1.62823 to 1.62646, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 1.6265 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6064\n",
      "Epoch 85: loss improved from 1.62646 to 1.60641, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 1.6064 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5902\n",
      "Epoch 86: loss improved from 1.60641 to 1.59024, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 1.5902 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5902\n",
      "Epoch 87: loss improved from 1.59024 to 1.59022, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 1.5902 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5732\n",
      "Epoch 88: loss improved from 1.59022 to 1.57318, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 168ms/step - loss: 1.5732 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5216\n",
      "Epoch 89: loss improved from 1.57318 to 1.52163, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 1.5216 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5008\n",
      "Epoch 90: loss improved from 1.52163 to 1.50079, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 156ms/step - loss: 1.5008 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4963\n",
      "Epoch 91: loss improved from 1.50079 to 1.49626, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 1.4963 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4908\n",
      "Epoch 92: loss improved from 1.49626 to 1.49082, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 1.4908 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4701\n",
      "Epoch 93: loss improved from 1.49082 to 1.47007, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 1.4701 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4854\n",
      "Epoch 94: loss did not improve from 1.47007\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 1.4854 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4783\n",
      "Epoch 95: loss did not improve from 1.47007\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 1.4783 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4811\n",
      "Epoch 96: loss did not improve from 1.47007\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 1.4811 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1052\n",
      "Epoch 97: loss improved from 1.47007 to 1.10519, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 1.1052 - lr: 2.0000e-04\n",
      "Epoch 98/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9672\n",
      "Epoch 98: loss improved from 1.10519 to 0.96718, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.9672 - lr: 2.0000e-04\n",
      "Epoch 99/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9166\n",
      "Epoch 99: loss improved from 0.96718 to 0.91657, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.9166 - lr: 2.0000e-04\n",
      "Epoch 100/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8904\n",
      "Epoch 100: loss improved from 0.91657 to 0.89037, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 0.8904 - lr: 2.0000e-04\n",
      "Epoch 101/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8753\n",
      "Epoch 101: loss improved from 0.89037 to 0.87531, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.8753 - lr: 2.0000e-04\n",
      "Epoch 102/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8626\n",
      "Epoch 102: loss improved from 0.87531 to 0.86258, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.8626 - lr: 2.0000e-04\n",
      "Epoch 103/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8565\n",
      "Epoch 103: loss improved from 0.86258 to 0.85645, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.8565 - lr: 2.0000e-04\n",
      "Epoch 104/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8460\n",
      "Epoch 104: loss improved from 0.85645 to 0.84599, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.8460 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8402\n",
      "Epoch 105: loss improved from 0.84599 to 0.84016, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.8402 - lr: 2.0000e-04\n",
      "Epoch 106/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8354\n",
      "Epoch 106: loss improved from 0.84016 to 0.83540, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.8354 - lr: 2.0000e-04\n",
      "Epoch 107/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8300\n",
      "Epoch 107: loss improved from 0.83540 to 0.83001, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 0.8300 - lr: 2.0000e-04\n",
      "Epoch 108/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8250\n",
      "Epoch 108: loss improved from 0.83001 to 0.82505, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.8250 - lr: 2.0000e-04\n",
      "Epoch 109/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8209\n",
      "Epoch 109: loss improved from 0.82505 to 0.82092, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 156ms/step - loss: 0.8209 - lr: 2.0000e-04\n",
      "Epoch 110/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8187\n",
      "Epoch 110: loss improved from 0.82092 to 0.81873, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.8187 - lr: 2.0000e-04\n",
      "Epoch 111/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8156\n",
      "Epoch 111: loss improved from 0.81873 to 0.81558, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.8156 - lr: 2.0000e-04\n",
      "Epoch 112/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8106\n",
      "Epoch 112: loss improved from 0.81558 to 0.81060, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 0.8106 - lr: 2.0000e-04\n",
      "Epoch 113/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8102\n",
      "Epoch 113: loss improved from 0.81060 to 0.81016, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 0.8102 - lr: 2.0000e-04\n",
      "Epoch 114/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8039\n",
      "Epoch 114: loss improved from 0.81016 to 0.80391, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.8039 - lr: 2.0000e-04\n",
      "Epoch 115/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8046\n",
      "Epoch 115: loss did not improve from 0.80391\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 0.8046 - lr: 2.0000e-04\n",
      "Epoch 116/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7998\n",
      "Epoch 116: loss improved from 0.80391 to 0.79980, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.7998 - lr: 2.0000e-04\n",
      "Epoch 117/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.8000\n",
      "Epoch 117: loss did not improve from 0.79980\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 118/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7941\n",
      "Epoch 118: loss improved from 0.79980 to 0.79407, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.7941 - lr: 2.0000e-04\n",
      "Epoch 119/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7944\n",
      "Epoch 119: loss did not improve from 0.79407\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.7944 - lr: 2.0000e-04\n",
      "Epoch 120/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7902\n",
      "Epoch 120: loss improved from 0.79407 to 0.79020, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.7902 - lr: 2.0000e-04\n",
      "Epoch 121/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7881\n",
      "Epoch 121: loss improved from 0.79020 to 0.78806, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.7881 - lr: 2.0000e-04\n",
      "Epoch 122/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7869\n",
      "Epoch 122: loss improved from 0.78806 to 0.78686, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.7869 - lr: 2.0000e-04\n",
      "Epoch 123/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7843\n",
      "Epoch 123: loss improved from 0.78686 to 0.78432, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.7843 - lr: 2.0000e-04\n",
      "Epoch 124/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7815\n",
      "Epoch 124: loss improved from 0.78432 to 0.78152, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 156ms/step - loss: 0.7815 - lr: 2.0000e-04\n",
      "Epoch 125/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7810\n",
      "Epoch 125: loss improved from 0.78152 to 0.78096, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.7810 - lr: 2.0000e-04\n",
      "Epoch 126/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7817\n",
      "Epoch 126: loss did not improve from 0.78096\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.7817 - lr: 2.0000e-04\n",
      "Epoch 127/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7786\n",
      "Epoch 127: loss improved from 0.78096 to 0.77857, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.7786 - lr: 2.0000e-04\n",
      "Epoch 128/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7733\n",
      "Epoch 128: loss improved from 0.77857 to 0.77330, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.7733 - lr: 2.0000e-04\n",
      "Epoch 129/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7765\n",
      "Epoch 129: loss did not improve from 0.77330\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.7765 - lr: 2.0000e-04\n",
      "Epoch 130/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7718\n",
      "Epoch 130: loss improved from 0.77330 to 0.77176, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.7718 - lr: 2.0000e-04\n",
      "Epoch 131/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7683\n",
      "Epoch 131: loss improved from 0.77176 to 0.76830, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.7683 - lr: 2.0000e-04\n",
      "Epoch 132/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7653\n",
      "Epoch 132: loss improved from 0.76830 to 0.76529, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 164ms/step - loss: 0.7653 - lr: 2.0000e-04\n",
      "Epoch 133/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7658\n",
      "Epoch 133: loss did not improve from 0.76529\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.7658 - lr: 2.0000e-04\n",
      "Epoch 134/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7638\n",
      "Epoch 134: loss improved from 0.76529 to 0.76383, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.7638 - lr: 2.0000e-04\n",
      "Epoch 135/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7648\n",
      "Epoch 135: loss did not improve from 0.76383\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.7648 - lr: 2.0000e-04\n",
      "Epoch 136/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7610\n",
      "Epoch 136: loss improved from 0.76383 to 0.76099, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.7610 - lr: 2.0000e-04\n",
      "Epoch 137/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7607\n",
      "Epoch 137: loss improved from 0.76099 to 0.76073, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.7607 - lr: 2.0000e-04\n",
      "Epoch 138/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7587\n",
      "Epoch 138: loss improved from 0.76073 to 0.75869, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 0.7587 - lr: 2.0000e-04\n",
      "Epoch 139/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7590\n",
      "Epoch 139: loss did not improve from 0.75869\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 0.7590 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7538\n",
      "Epoch 140: loss improved from 0.75869 to 0.75381, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.7538 - lr: 2.0000e-04\n",
      "Epoch 141/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7547\n",
      "Epoch 141: loss did not improve from 0.75381\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.7547 - lr: 2.0000e-04\n",
      "Epoch 142/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7546\n",
      "Epoch 142: loss did not improve from 0.75381\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 0.7546 - lr: 2.0000e-04\n",
      "Epoch 143/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7511\n",
      "Epoch 143: loss improved from 0.75381 to 0.75106, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.7511 - lr: 2.0000e-04\n",
      "Epoch 144/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7498\n",
      "Epoch 144: loss improved from 0.75106 to 0.74981, saving model to nextword1.h5\n",
      "61/61 [==============================] - 10s 164ms/step - loss: 0.7498 - lr: 2.0000e-04\n",
      "Epoch 145/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7479\n",
      "Epoch 145: loss improved from 0.74981 to 0.74794, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.7479 - lr: 2.0000e-04\n",
      "Epoch 146/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7473\n",
      "Epoch 146: loss improved from 0.74794 to 0.74730, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.7473 - lr: 2.0000e-04\n",
      "Epoch 147/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7446\n",
      "Epoch 147: loss improved from 0.74730 to 0.74464, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.7446 - lr: 2.0000e-04\n",
      "Epoch 148/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7439\n",
      "Epoch 148: loss improved from 0.74464 to 0.74388, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.7439 - lr: 2.0000e-04\n",
      "Epoch 149/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7432\n",
      "Epoch 149: loss improved from 0.74388 to 0.74316, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.7432 - lr: 2.0000e-04\n",
      "Epoch 150/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7408\n",
      "Epoch 150: loss improved from 0.74316 to 0.74083, saving model to nextword1.h5\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.7408 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b781bc6d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam')\n",
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cdf664",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342a03f",
   "metadata": {},
   "source": [
    "The loss significantly improved from 7.33 to 0.74 over 150 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f0dfb",
   "metadata": {},
   "source": [
    "The learning rate becomes negligible after epoch 97. Which means the model has reached it's maximum capacity at 97th epoch itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46e580",
   "metadata": {},
   "source": [
    "As there is a steady decline in loss, this is a good model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
